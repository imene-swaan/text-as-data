{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NewsCategorizer.csv')\n",
    "\n",
    "df = df[['category', 'short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Many fans were pissed after seeing the minor l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Never change, young man. Never change.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Wallace was hit with a first technical for a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>They believe CBD could be an alternative to po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The gymnast is in a league of her own.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                  short_description\n",
       "0      WELLNESS  Resting is part of training. I've confirmed wh...\n",
       "1      WELLNESS  Think of talking to yourself as a tool to coac...\n",
       "2      WELLNESS  The clock is ticking for the United States to ...\n",
       "3      WELLNESS  If you want to be busy, keep trying to be perf...\n",
       "4      WELLNESS  First, the bad news: Soda bread, corned beef a...\n",
       "...         ...                                                ...\n",
       "49995    SPORTS  Many fans were pissed after seeing the minor l...\n",
       "49996    SPORTS             Never change, young man. Never change.\n",
       "49997    SPORTS  Wallace was hit with a first technical for a h...\n",
       "49998    SPORTS  They believe CBD could be an alternative to po...\n",
       "49999    SPORTS             The gymnast is in a league of her own.\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_punctuation = '!\"$%&#()*+,-./:;<=>?[\\\\]^_`{|}~•'\n",
    "    \n",
    "def preprocess(text_string):\n",
    "    space_pattern = '\\s+'\n",
    "    new_line = '\\n+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    non_word_char = '[^\\w]'\n",
    "    underscore = '_[\\w]+'\n",
    "    \n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(new_line, ' ', parsed_text)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(non_word_char, ' ', parsed_text)\n",
    "    parsed_text = re.sub(r\"\\bو(.*?)\\b\", r'\\1', parsed_text)\n",
    "    parsed_text = re.sub('([0-9]+)', '', parsed_text)\n",
    "    parsed_text = re.sub(underscore, ' ', parsed_text)\n",
    "    \n",
    "    return parsed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/imenekolli/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/imenekolli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy'] = np.vectorize(preprocess)(df['short_description'])\n",
    "df['tidy'] = np.vectorize(deEmojify)(df['tidy'] )\n",
    "\n",
    "df['tidy'] = df['tidy'].str.strip()\n",
    "df['tidy'] = df['tidy'].apply(lambda x: str.lower(x))\n",
    "df['tidy'] = df['tidy'].apply(lambda x: x.split())\n",
    "\n",
    "df['tidy'] = df['tidy'].apply(lambda x: list(map(lambda y: lemma.lemmatize(y, pos =\"n\"), x)))\n",
    "df['tidy'] = df['tidy'].apply(lambda x: list(map(lambda y: lemma.lemmatize(y, pos =\"v\"), x)))\n",
    "df['tidy'] = df['tidy'].apply(lambda x: list(map(lambda y: lemma.lemmatize(y, pos =\"a\"), x)))\n",
    "\n",
    "df['tidy'] = df['tidy'].apply(lambda x: list(filter(lambda y: not y in stop_words, x)))\n",
    "\n",
    "df['tidy'] = df['tidy'].apply(lambda x: list(filter(lambda y: not len(y) < 3 , x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['draw',\n",
       " 'barrymore',\n",
       " 'announce',\n",
       " 'joyous',\n",
       " 'news',\n",
       " 'week',\n",
       " 'pregnant',\n",
       " 'second',\n",
       " 'child',\n",
       " 'similar',\n",
       " 'baby',\n",
       " 'news',\n",
       " 'gwen',\n",
       " 'stefani']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tidy[14583]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>tidy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "      <td>[rest, part, train, confirm, sort, already, kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "      <td>[think, talk, tool, coach, challenge, narrate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "      <td>[clock, tick, unite, state, find, cure, team, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "      <td>[want, busy, keep, try, perfect, want, happy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "      <td>[first, bad, news, soda, bread, corn, beef, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Many fans were pissed after seeing the minor l...</td>\n",
       "      <td>[many, fan, piss, see, minor, league, team, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Never change, young man. Never change.</td>\n",
       "      <td>[never, change, young, man, never, change]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Wallace was hit with a first technical for a h...</td>\n",
       "      <td>[wallace, hit, first, technical, hard, foul, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>They believe CBD could be an alternative to po...</td>\n",
       "      <td>[believe, cbd, could, alternative, potent, pai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The gymnast is in a league of her own.</td>\n",
       "      <td>[gymnast, league]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                  short_description  \\\n",
       "0      WELLNESS  Resting is part of training. I've confirmed wh...   \n",
       "1      WELLNESS  Think of talking to yourself as a tool to coac...   \n",
       "2      WELLNESS  The clock is ticking for the United States to ...   \n",
       "3      WELLNESS  If you want to be busy, keep trying to be perf...   \n",
       "4      WELLNESS  First, the bad news: Soda bread, corned beef a...   \n",
       "...         ...                                                ...   \n",
       "49995    SPORTS  Many fans were pissed after seeing the minor l...   \n",
       "49996    SPORTS             Never change, young man. Never change.   \n",
       "49997    SPORTS  Wallace was hit with a first technical for a h...   \n",
       "49998    SPORTS  They believe CBD could be an alternative to po...   \n",
       "49999    SPORTS             The gymnast is in a league of her own.   \n",
       "\n",
       "                                                    tidy  \n",
       "0      [rest, part, train, confirm, sort, already, kn...  \n",
       "1      [think, talk, tool, coach, challenge, narrate,...  \n",
       "2      [clock, tick, unite, state, find, cure, team, ...  \n",
       "3      [want, busy, keep, try, perfect, want, happy, ...  \n",
       "4      [first, bad, news, soda, bread, corn, beef, be...  \n",
       "...                                                  ...  \n",
       "49995  [many, fan, piss, see, minor, league, team, of...  \n",
       "49996         [never, change, young, man, never, change]  \n",
       "49997  [wallace, hit, first, technical, hard, foul, l...  \n",
       "49998  [believe, cbd, could, alternative, potent, pai...  \n",
       "49999                                  [gymnast, league]  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    use_idf=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_untokenized'] = df['tidy'].apply(lambda x: ' '.join([w for w in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 31222)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(df['tidy_untokenized'])\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aafa</th>\n",
       "      <th>aam</th>\n",
       "      <th>aamer</th>\n",
       "      <th>aan</th>\n",
       "      <th>aanav</th>\n",
       "      <th>aap</th>\n",
       "      <th>aarhus</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aarp</th>\n",
       "      <th>...</th>\n",
       "      <th>état</th>\n",
       "      <th>étienne</th>\n",
       "      <th>être</th>\n",
       "      <th>île</th>\n",
       "      <th>övertorneå</th>\n",
       "      <th>öztürk</th>\n",
       "      <th>über</th>\n",
       "      <th>łowicz</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬂavors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 31222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aaa  aafa  aam  aamer  aan  aanav  aap  aarhus  aaron  aarp  ...  état  \\\n",
       "0      0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "1      0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "2      0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "3      0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "4      0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "...    ...   ...  ...    ...  ...    ...  ...     ...    ...   ...  ...   ...   \n",
       "49995  0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "49996  0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "49997  0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "49998  0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "49999  0.0   0.0  0.0    0.0  0.0    0.0  0.0     0.0    0.0   0.0  ...   0.0   \n",
       "\n",
       "       étienne  être  île  övertorneå  öztürk  über  łowicz  ﬁrst  ﬂavors  \n",
       "0          0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "1          0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "2          0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "3          0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "4          0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "...        ...   ...  ...         ...     ...   ...     ...   ...     ...  \n",
       "49995      0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "49996      0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "49997      0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "49998      0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "49999      0.0   0.0  0.0         0.0     0.0   0.0     0.0   0.0     0.0  \n",
       "\n",
       "[50000 rows x 31222 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = pd.DataFrame(tfidf.toarray(), columns = tfidf_vectorizer.get_feature_names_out())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = np.array(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x in stop_words, tfidf_vectorizer.get_feature_names_out())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>tidy</th>\n",
       "      <th>tidy_untokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "      <td>[rest, part, train, confirm, sort, already, kn...</td>\n",
       "      <td>rest part train confirm sort already know buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "      <td>[think, talk, tool, coach, challenge, narrate,...</td>\n",
       "      <td>think talk tool coach challenge narrate experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "      <td>[clock, tick, unite, state, find, cure, team, ...</td>\n",
       "      <td>clock tick unite state find cure team work stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "      <td>[want, busy, keep, try, perfect, want, happy, ...</td>\n",
       "      <td>want busy keep try perfect want happy focus ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "      <td>[first, bad, news, soda, bread, corn, beef, be...</td>\n",
       "      <td>first bad news soda bread corn beef beer highl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Many fans were pissed after seeing the minor l...</td>\n",
       "      <td>[many, fan, piss, see, minor, league, team, of...</td>\n",
       "      <td>many fan piss see minor league team offensive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Never change, young man. Never change.</td>\n",
       "      <td>[never, change, young, man, never, change]</td>\n",
       "      <td>never change young man never change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Wallace was hit with a first technical for a h...</td>\n",
       "      <td>[wallace, hit, first, technical, hard, foul, l...</td>\n",
       "      <td>wallace hit first technical hard foul luis sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>They believe CBD could be an alternative to po...</td>\n",
       "      <td>[believe, cbd, could, alternative, potent, pai...</td>\n",
       "      <td>believe cbd could alternative potent painkille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The gymnast is in a league of her own.</td>\n",
       "      <td>[gymnast, league]</td>\n",
       "      <td>gymnast league</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                  short_description  \\\n",
       "0      WELLNESS  Resting is part of training. I've confirmed wh...   \n",
       "1      WELLNESS  Think of talking to yourself as a tool to coac...   \n",
       "2      WELLNESS  The clock is ticking for the United States to ...   \n",
       "3      WELLNESS  If you want to be busy, keep trying to be perf...   \n",
       "4      WELLNESS  First, the bad news: Soda bread, corned beef a...   \n",
       "...         ...                                                ...   \n",
       "49995    SPORTS  Many fans were pissed after seeing the minor l...   \n",
       "49996    SPORTS             Never change, young man. Never change.   \n",
       "49997    SPORTS  Wallace was hit with a first technical for a h...   \n",
       "49998    SPORTS  They believe CBD could be an alternative to po...   \n",
       "49999    SPORTS             The gymnast is in a league of her own.   \n",
       "\n",
       "                                                    tidy  \\\n",
       "0      [rest, part, train, confirm, sort, already, kn...   \n",
       "1      [think, talk, tool, coach, challenge, narrate,...   \n",
       "2      [clock, tick, unite, state, find, cure, team, ...   \n",
       "3      [want, busy, keep, try, perfect, want, happy, ...   \n",
       "4      [first, bad, news, soda, bread, corn, beef, be...   \n",
       "...                                                  ...   \n",
       "49995  [many, fan, piss, see, minor, league, team, of...   \n",
       "49996         [never, change, young, man, never, change]   \n",
       "49997  [wallace, hit, first, technical, hard, foul, l...   \n",
       "49998  [believe, cbd, could, alternative, potent, pai...   \n",
       "49999                                  [gymnast, league]   \n",
       "\n",
       "                                        tidy_untokenized  \n",
       "0      rest part train confirm sort already know buil...  \n",
       "1      think talk tool coach challenge narrate experi...  \n",
       "2      clock tick unite state find cure team work stu...  \n",
       "3      want busy keep try perfect want happy focus ma...  \n",
       "4      first bad news soda bread corn beef beer highl...  \n",
       "...                                                  ...  \n",
       "49995  many fan piss see minor league team offensive ...  \n",
       "49996                never change young man never change  \n",
       "49997  wallace hit first technical hard foul luis sco...  \n",
       "49998  believe cbd could alternative potent painkille...  \n",
       "49999                                     gymnast league  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WELLNESS': 0,\n",
       " 'POLITICS': 1,\n",
       " 'ENTERTAINMENT': 2,\n",
       " 'TRAVEL': 3,\n",
       " 'STYLE & BEAUTY': 4,\n",
       " 'PARENTING': 5,\n",
       " 'FOOD & DRINK': 6,\n",
       " 'WORLD NEWS': 7,\n",
       " 'BUSINESS': 8,\n",
       " 'SPORTS': 9}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = {}\n",
    "for i in range(len(df['category'].unique())):\n",
    "    classes[df['category'].unique()[i]] = i \n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['category'].map(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 9367)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "new_tfidf = SelectPercentile(chi2, percentile=30).fit_transform(tfidf, df['class'])\n",
    "new_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics or components\n",
    "num_components=10\n",
    "\n",
    "# Create SVD object\n",
    "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
    "\n",
    "# Fit SVD model on data\n",
    "lsa.fit_transform(new_tfidf)\n",
    "\n",
    "# Get Singular values and Components \n",
    "Sigma = lsa.singular_values_ \n",
    "V_transpose = lsa.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['continent', 'expertise', 'elayna', 'bradshaw', 'church']\n",
      "Topic 1:  ['armor', 'ethic', 'enculturate', 'bjergsø', 'durbin']\n",
      "Topic 2:  ['expertise', 'contaminant', 'condescendingly', 'channing', 'bnbfinder']\n",
      "Topic 3:  ['devine', 'couric', 'continent', 'existential', 'draft']\n",
      "Topic 4:  ['bradshaw', 'devine', 'elayna', 'dazs', 'couric']\n",
      "Topic 5:  ['continent', 'bradshaw', 'balmy', 'egocentric', 'encyclical']\n",
      "Topic 6:  ['elayna', 'condescendingly', 'earth', 'bnbfinder', 'balmy']\n",
      "Topic 7:  ['chinasmack', 'breathtaking', 'cheng', 'continent', 'devine']\n",
      "Topic 8:  ['elayna', 'egon', 'expertise', 'centro', 'contaminant']\n",
      "Topic 9:  ['elayna', 'church', 'devine', 'breathtaking', 'continent']\n"
     ]
    }
   ],
   "source": [
    "# Print the topics with their terms\n",
    "terms = feature_array\n",
    "\n",
    "for index, component in enumerate(lsa.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WELLNESS', 'POLITICS', 'ENTERTAINMENT', 'TRAVEL',\n",
       "       'STYLE & BEAUTY', 'PARENTING', 'FOOD & DRINK', 'WORLD NEWS',\n",
       "       'BUSINESS', 'SPORTS'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Genism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>tidy</th>\n",
       "      <th>tidy_untokenized</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "      <td>[rest, part, train, confirm, sort, already, kn...</td>\n",
       "      <td>rest part train confirm sort already know buil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "      <td>[think, talk, tool, coach, challenge, narrate,...</td>\n",
       "      <td>think talk tool coach challenge narrate experi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "      <td>[clock, tick, unite, state, find, cure, team, ...</td>\n",
       "      <td>clock tick unite state find cure team work stu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "      <td>[want, busy, keep, try, perfect, want, happy, ...</td>\n",
       "      <td>want busy keep try perfect want happy focus ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "      <td>[first, bad, news, soda, bread, corn, beef, be...</td>\n",
       "      <td>first bad news soda bread corn beef beer highl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Many fans were pissed after seeing the minor l...</td>\n",
       "      <td>[many, fan, piss, see, minor, league, team, of...</td>\n",
       "      <td>many fan piss see minor league team offensive ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Never change, young man. Never change.</td>\n",
       "      <td>[never, change, young, man, never, change]</td>\n",
       "      <td>never change young man never change</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Wallace was hit with a first technical for a h...</td>\n",
       "      <td>[wallace, hit, first, technical, hard, foul, l...</td>\n",
       "      <td>wallace hit first technical hard foul luis sco...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>They believe CBD could be an alternative to po...</td>\n",
       "      <td>[believe, cbd, could, alternative, potent, pai...</td>\n",
       "      <td>believe cbd could alternative potent painkille...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The gymnast is in a league of her own.</td>\n",
       "      <td>[gymnast, league]</td>\n",
       "      <td>gymnast league</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                  short_description  \\\n",
       "0      WELLNESS  Resting is part of training. I've confirmed wh...   \n",
       "1      WELLNESS  Think of talking to yourself as a tool to coac...   \n",
       "2      WELLNESS  The clock is ticking for the United States to ...   \n",
       "3      WELLNESS  If you want to be busy, keep trying to be perf...   \n",
       "4      WELLNESS  First, the bad news: Soda bread, corned beef a...   \n",
       "...         ...                                                ...   \n",
       "49995    SPORTS  Many fans were pissed after seeing the minor l...   \n",
       "49996    SPORTS             Never change, young man. Never change.   \n",
       "49997    SPORTS  Wallace was hit with a first technical for a h...   \n",
       "49998    SPORTS  They believe CBD could be an alternative to po...   \n",
       "49999    SPORTS             The gymnast is in a league of her own.   \n",
       "\n",
       "                                                    tidy  \\\n",
       "0      [rest, part, train, confirm, sort, already, kn...   \n",
       "1      [think, talk, tool, coach, challenge, narrate,...   \n",
       "2      [clock, tick, unite, state, find, cure, team, ...   \n",
       "3      [want, busy, keep, try, perfect, want, happy, ...   \n",
       "4      [first, bad, news, soda, bread, corn, beef, be...   \n",
       "...                                                  ...   \n",
       "49995  [many, fan, piss, see, minor, league, team, of...   \n",
       "49996         [never, change, young, man, never, change]   \n",
       "49997  [wallace, hit, first, technical, hard, foul, l...   \n",
       "49998  [believe, cbd, could, alternative, potent, pai...   \n",
       "49999                                  [gymnast, league]   \n",
       "\n",
       "                                        tidy_untokenized  class  \n",
       "0      rest part train confirm sort already know buil...      0  \n",
       "1      think talk tool coach challenge narrate experi...      0  \n",
       "2      clock tick unite state find cure team work stu...      0  \n",
       "3      want busy keep try perfect want happy focus ma...      0  \n",
       "4      first bad news soda bread corn beef beer highl...      0  \n",
       "...                                                  ...    ...  \n",
       "49995  many fan piss see minor league team offensive ...      9  \n",
       "49996                never change young man never change      9  \n",
       "49997  wallace hit first technical hard foul luis sco...      9  \n",
       "49998  believe cbd could alternative potent painkille...      9  \n",
       "49999                                     gymnast league      9  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.305*\"one\" + 0.277*\"year\" + 0.251*\"time\" + 0.236*\"make\" + 0.208*\"get\"'), (1, '0.799*\"year\" + -0.329*\"one\" + 0.222*\"new\" + -0.204*\"make\" + 0.181*\"old\"'), (2, '-0.850*\"one\" + 0.269*\"make\" + 0.199*\"get\" + -0.193*\"year\" + 0.179*\"time\"'), (3, '0.806*\"make\" + -0.404*\"time\" + -0.323*\"get\" + 0.197*\"year\" + -0.065*\"day\"'), (4, '0.692*\"time\" + -0.632*\"get\" + -0.209*\"new\" + 0.106*\"make\" + -0.085*\"year\"'), (5, '0.822*\"new\" + -0.307*\"get\" + -0.304*\"year\" + 0.171*\"york\" + -0.118*\"day\"'), (6, '-0.490*\"get\" + -0.364*\"time\" + 0.352*\"like\" + -0.335*\"make\" + 0.220*\"say\"'), (7, '-0.413*\"want\" + -0.362*\"check\" + -0.320*\"sure\" + -0.296*\"twitter\" + -0.291*\"facebook\"'), (8, '0.764*\"day\" + -0.366*\"like\" + 0.323*\"take\" + -0.195*\"know\" + -0.144*\"time\"'), (9, '-0.701*\"child\" + 0.398*\"like\" + -0.353*\"parent\" + -0.282*\"life\" + 0.185*\"look\"')]\n"
     ]
    }
   ],
   "source": [
    "# LSA Model\n",
    "number_of_topics=10\n",
    "words=10\n",
    "clean_text= df['tidy']\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('imenepy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a504093cd48dbf41332b0076d682ce93e05d52a892a597f2a787966ee958626a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
